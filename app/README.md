## Overview 

This section holds all of the projects python scripts that will be needed to convert the models, run performance testing, and all other functionality. This will grow as the project expands.

## Milestones
This section will display the milestones and also tell which files satisfy each milestone and how. 

### Phase 1 (Resource Contraints, Models, and Exporting to Onnx)

#### Selected Models 
1. CompVis/stable-diffusion-v1-4 for Stable Diffusion and Image generation
2. openai-community/gpt2-medium for Text prompt Generation

#### Resource Contraints 
1. Due to needed memory/cpu usage and size of the stable-diffusion model resource contraints include available RAM and CPU as long as how much space we need on the device to store the models. 

#### Possible Optimizations
Since we are concerned with the size of the model along with the needed memory/cpu usage we can do a few things to help with this. 
1. Model Quantization to reduce the size of the models.
2. Pruning/KV cachine for gpt model for better optimization
3. Reducing UNET timesteps and sample sizing to reduce cpu and memory load (tradeoff is reduced quality in generated image).

#### General design of full demo
1. Export Models to Onnx
2. Ask the User to input a word or phrase that they want to be generated into a creative prompt
3. The prompt is outputted and then sent to the image generator
4. The image is generated by the stable diffusion model
5. The user is then prompted to choose which item they want to edit from the picture
6. Image detection model creates a mask of the image.
7. User is prompted to tell what they want the edit to do (Ex. Change Color)
8. Picture is edited and saved for user to view.

#### Files that contain benchmarking and onnx export for 1st Milestone.
1. stable_diffusion_to_onnx_64px_quantized.py
2. gpt2_pruned_vs_base.py
3. *** convert_gpt2_to_onnx.py 

Both of these first two files have basic onnx exporting and benchmarking and then compares them against the optimized versions as well. These started out as the original onnx export and benchmarking but have changed throughout the process to now contain most of the milestones goals in each one. 

*** convert_gpt2_to_onnx.py is one of the first tests I created and does display a nice UI output of benchmarking so I will also add that here (it is also displayed below in the current files tab) but it is not as far along as the other two. 

### Phase 2-3 (Optimization for text generation and image generation models)
I grouped these together as it worked out for me that my first two selected models were actually needed for these sections. So I will discuss some of the steps I took and the outputs.

#### Files that contain Optimization and benchmarking
1. stable_diffusion_to_onnx_64px_quantized.py
2. gpt2_pruned_vs_base.py

As I said before both of these files have been updated throughout the process so they again pertain to this certain phase. 

##### Optimizing Text generation model (GPT2)
For the text generation model I used pruning and kv caching to optimize the text generation in this file and then compared the outputs of both to compare the results.

- Pruning:

  ![image](https://github.com/user-attachments/assets/91e0e06b-1d43-4d11-ba07-dcd93498846b)

-KV caching:

  ![image](https://github.com/user-attachments/assets/58790934-523b-43f7-91da-a86e8ffc04d5)

Both of these optimizations did help in terms of speed and resources but it badly affected the quality of the generated text from what I saw. Using no optimization techniques my text generation stayed on track and produced rather good creative outputs but with the optimization it struggled to stay on topic and usually drifted into typing nonsense. I even tried some repetition handling and warnings where it would basically tell the model it was wrong for having repition in its outputs and that still did not help. That is why these files are seperate from the full demo as the tradeoff for the functionality was not worth it from what I saw in my examples. So, I decided to stick with the base model with no optimization for the full demo but did keep them  in these other filesfor benchmarking purposes. 

Example Benchmark output from gpt2_pruned_vs_base.py:


##### Optimizing Stable Diffusion Image Generator
For the image generation model I used quantization and then also downgraded some of the UNET model's values to help with the optimization here.

- Quantization:

  ![image](https://github.com/user-attachments/assets/ad6c2fd6-88c9-4d8a-b737-52e98d3f73a3)

  Due to the stable diffusion model having separate pipelines we have to quantize each one separatley (text_encoder, unet, vae). So this is an example of the code calling each one to quantize them. 

  ![image](https://github.com/user-attachments/assets/a2c068ba-426a-4215-a172-9a2fb9c256c4)

  Each one calls the quantize static function (quantize dynamic was causing issues so swapped) and I am created calibration data for each input as well

  ![image](https://github.com/user-attachments/assets/6e2a32d2-d411-422f-971f-5f6c64c114c3)

- Unet values:
  I downgraded the UNET values to help with cpu/memory usage as well. I changed the timesteps to be 50 and the sample size down to 64x64 to help with the resources.

 Once again using these techniques I was able to reduce resources needed and also reduce the size of the model files but the tradeoff was that the quality of the image that was generated was greatly affected. With quantization the image looked like smeared paint and     then without quantization and just the 64x64 sample size would create a fairly decent image but again at a drastically reduced quality. In the stable_diffusion_64px_quantized.py file I have kept the quantization for benchmarking to compare the regular to quantized    models but in the full demo I made the trade off of no quantization but kept the 64x64 sample size as the image is viewable and does not take 20 minutes plus to create. 

 Example output from stable_diffusion_to_onnx_64px_quantized.py:
 
  ![image](https://github.com/user-attachments/assets/762618c1-604b-4c54-9b56-2243d42d3c64)

  ![image](https://github.com/user-attachments/assets/e8207ba5-a2ec-49d7-90e6-b10b929b1f3b)

  ![image](https://github.com/user-attachments/assets/7a0219a9-11b2-4959-bbf3-bed62d73ca56)

  These will be saved after running the code to the /benchmark/diffusion location.


  

 

### Phase 4-End (Image detection and Final Demo)
- For my image detection/masking (as I needed one to make a mask for my edit process) I am using the CIDAS/clipseg-rd64-refined model.

#### Files that contain these phases
1. Full_demo.py

#### Image detection/Masking 
- In my full_demo.py file After I generate the image I am using the clipseg model to take a user prompt to create a mask based on what they give it:

![image](https://github.com/user-attachments/assets/72c4e48a-1799-4edc-8576-47a0dca73605)

So if the user gives it the prompt of "sky" it will take the generated image and then create an inferno mask that we then send to be edited:

![image](https://github.com/user-attachments/assets/01b29ae6-47ab-49cb-b080-ce4e179d8ae5)

So you can see it takes the original image, creates the inferno mask, and then uses that to determine what area needs to be edited all based on the prompt "sky" to the clipseg model. 


#### Full demo 
- The full demo outline is as follows:
1. The models are exported to onnx
2. The user is prompted to input a key word or phrase that will turned into a creative prompt.
3. GPT2 generates 3 batches prompts that are then given to the user to select which one they like the best.
4. On selection it sends the prompt to the image generator and creates the image.
5. On completion of the image the user is then asked what they want to edit in the photo (ex. Sky,water,tree)
6. After that prompt it will ask what the user wants to do to the selection (ex. change color to x)
7. The clipseg model will take this prompt and create an inferno mask of the item selected and pass that to our edit function.
8. The edit function will take the user prompted color and blend that color change into the mask and then re-show the photo.
9. At the end an output is given to show the difference between the original and edited photo. As well as a "quality_photo" that I generated at better resolution just to show what the edit looks like on a very good quality photo.

Full demo output picture: 

![image](https://github.com/user-attachments/assets/0cc33dc2-5984-49e8-aca8-3736286c5b08)

The user prompts are shown at the top as well.



## Current files

### convert_gpt2_to_onnx.py
This script converts the openai-community/gpt2 huggingface model to onnx and then runs through 100 tests to create a performance report so that we  can see exactly how the model is performing on the system.

Example output:
![image](https://github.com/user-attachments/assets/dc48c495-2554-49f7-a1de-810498503b82)


### gpt2_pruned_vs_base.py
This script converts the openai-community/gpt2-medium huggingface model to onnx and then runs through kv caching and pruning optimization techniques and displays the differences between the base and pruned benchmarking. 

Example output:


### stable_diffusion_to_onnx_64px_quantized.py
This script converts the CompVis/stable-diffusion-v1-4 huggingface model to onnx and then runs through basic quantization techniques and then displays the differences between the base and quantized benchmarking.

Example output: 

![image](https://github.com/user-attachments/assets/2b3a691a-e270-4ae9-93bf-26e170508acd)

![image](https://github.com/user-attachments/assets/5d60bee3-77e6-461c-b2f1-049d3992c1d6)

![image](https://github.com/user-attachments/assets/6192a934-1977-4057-ae12-93a8275be00f)

These tables and graphs are saved in the /benchmark/diffusion location: 

![image](https://github.com/user-attachments/assets/2c3fa3c2-c581-4655-af41-407d3dd00b98)

The rest can be found in the console UI as well. 




